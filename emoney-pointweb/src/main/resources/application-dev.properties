#shardingsphere配置
spring.shardingsphere.props.sql-show=true
spring.shardingsphere.datasource.names=db0
spring.shardingsphere.datasource.common.driver-class-name=com.microsoft.sqlserver.jdbc.SQLServerDriver
spring.shardingsphere.datasource.common.type=com.alibaba.druid.pool.DruidDataSource
spring.shardingsphere.datasource.db0.url=jdbc:sqlserver://192.168.42.79:1433;DatabaseName=pointprod
spring.shardingsphere.datasource.db0.username=sa
spring.shardingsphere.datasource.db0.password=1qaz=[.
#t_point_record分表
spring.shardingsphere.rules.sharding.sharding-algorithms.pointreocrd.type=INLINE
spring.shardingsphere.rules.sharding.sharding-algorithms.pointreocrd.props.algorithm-expression=t_point_record$->{uid % 32}
spring.shardingsphere.rules.sharding.tables.t_point_record.actual-data-nodes=db0.t_point_record$->{0..31}
spring.shardingsphere.rules.sharding.tables.t_point_record.table-strategy.standard.sharding-column=uid
spring.shardingsphere.rules.sharding.tables.t_point_record.table-strategy.standard.sharding-algorithm-name=pointreocrd
#_signin_record分表
spring.shardingsphere.rules.sharding.sharding-algorithms.signinrecord.type=INLINE
spring.shardingsphere.rules.sharding.sharding-algorithms.signinrecord.props.algorithm-expression=t_signin_record$->{uid % 32}
spring.shardingsphere.rules.sharding.tables.t_signin_record.actual-data-nodes=db0.t_signin_record$->{0..31}
spring.shardingsphere.rules.sharding.tables.t_signin_record.table-strategy.standard.sharding-column=uid
spring.shardingsphere.rules.sharding.tables.t_signin_record.table-strategy.standard.sharding-algorithm-name=signinrecord

spring.shardingsphere.rules.sharding.key-generators.snowflake.type=SNOWFLAKE
spring.shardingsphere.rules.sharding.key-generators.snowflake.props.work-id=1

#druid连接池配置
spring.datasource.druid.initial-size=5
spring.datasource.druid.min-idle=5
spring.datasource.druid.max-active=20
spring.datasource.druid.max-wait=60000
spring.datasource.druid.time-between-eviction-runs-millis=60000
spring.datasource.druid.min-evictable-idle-time-millis=300000
spring.datasource.druid.validation-query=SELECT 1 
spring.datasource.druid.test-while-idle=true
spring.datasource.druid.test-on-borrow=false
spring.datasource.druid.test-on-return=false
spring.datasource.druid.pool-prepared-statements=true
spring.datasource.druid.max-pool-prepared-statement-per-connection-size=20
spring.datasource.druid.filter.commons-log.connection-logger-name=stat,wall,log4j
spring.datasource.druid.filter.stat.log-slow-sql=true
spring.datasource.druid.filter.stat.slow-sql-millis=2000
spring.datasource.druid.connect-properties.=druid.stat.mergeSql=true;druid.stat.slowSqlMillis=5000
spring.datasource.druid.use-global-data-source-stat=true
#pagehelper插件配置
pagehelper.reasonable=true
pagehelper.support-methods-arguments=true
pagehelper.helper-dialect=sqlserver
pagehelper.params=count=countsql
pagehelper.pageSizeZero=true
#mybatis配置
mybatis.config-location=classpath:mybatis/mybatis-config.xml
mybatis.mapper-locations=classpath:mybatis/mapper/*.xml
mybatis.type-aliases-package=com.emoney.pointweb.repository.dao.entity
### resources
spring.mvc.servlet.load-on-startup=0
spring.mvc.static-path-pattern=/static/**
spring.resources.static-locations=classpath:/static/
### freemarker
spring.freemarker.templateLoaderPath=classpath:/templates/
spring.freemarker.suffix=.ftl
spring.freemarker.charset=UTF-8
spring.freemarker.request-context-attribute=request
spring.freemarker.settings.number_format=0.##########
#redis 单实例多数据源以及lettuce连接池配置
spring.redis.lettuce.pool.maxTotal=50
spring.redis.lettuce.pool.maxWaitMillis=-1
spring.redis.lettuce.pool.maxIdle=8
spring.redis.lettuce.pool.minIdle=0
#redis1
spring.redis1.database=0
spring.redis1.hostName=192.168.8.175
spring.redis1.port=6479
spring.redis1.password=
spring.redis1.timeout=600
#kafka配置
spring.kafka.bootstrap-servers=192.168.42.172:9092,192.168.42.173:9092,192.168.42.174:9092
# 发生错误后，消息重发的次数。
spring.kafka.producer.retries=0
#当有多个消息需要被发送到同一个分区时，生产者会把它们放在同一个批次里。该参数指定了一个批次可以使用的内存大小，按照字节数计算。
spring.kafka.producer.batch-size=16384
# 设置生产者内存缓冲区的大小。
spring.kafka.producer.buffer-memory=33554432
# 每个Batch要存放batch.size大小的数据后，才可以发送出去
spring.kafka.producer.linger.ms=0
# 键的序列化方式
spring.kafka.producer.key-serializer=org.apache.kafka.common.serialization.StringSerializer
# 值的序列化方式
spring.kafka.producer.value-serializer=org.apache.kafka.common.serialization.StringSerializer
# acks=0 ： 生产者在成功写入消息之前不会等待任何来自服务器的响应。
# acks=1 ： 只要集群的首领节点收到消息，生产者就会收到一个来自服务器成功响应。
# acks=all ：只有当所有参与复制的节点全部收到消息时，生产者才会收到一个来自服务器的成功响应。
spring.kafka.producer.acks=1
# 自动提交的时间间隔
spring.kafka.consumer.auto-commit-interval=100
# 该属性指定了消费者在读取一个没有偏移量的分区或者偏移量无效的情况下该作何处理：
# latest（默认值）在偏移量无效的情况下，消费者将从最新的记录开始读取数据（在消费者启动之后生成的记录）
# earliest ：在偏移量无效的情况下，消费者将从起始位置读取分区的记录
spring.kafka.consumer.auto-offset-reset=latest
# 是否自动提交偏移量，默认值是true,为了避免出现重复数据和数据丢失，可以把它设置为false,然后手动提交偏移量
spring.kafka.consumer.enable-auto-commit=false
# 每次最大消费消息数量
spring.kafka.consumer.max-poll-records=50
# 键的反序列化方式
spring.kafka.consumer.key-deserializer=org.apache.kafka.common.serialization.StringDeserializer
# 值的反序列化方式
spring.kafka.consumer.value-deserializer=org.apache.kafka.common.serialization.StringDeserializer
# 消费会话超时时间(超过这个时间consumer没有发送心跳,就会触发rebalance操作)
spring.kafka.consumer.session.timeout.ms=120000
# 消费请求超时时间
spring.kafka.consumer.request.timeout.ms=180000
#消费分组
spring.kafka.consumer.topic.group.id=pointrecordgroup
# 在侦听器容器中运行的线程数。
spring.kafka.listener.concurrency=6
#listner负责ack，每调用一次，就立即commit
spring.kafka.listener.ack-mode=manual_immediate
# 设置批量消费
# spring.kafka.listener.type=batch
# 批量消费每次最多消费多少条消息
# spring.kafka.consumer.max-poll-records=50
# 消费端监听的topic不存在时，项目启动会报错(关掉)
spring.kafka.listener.missing-topics-fatal=false
#elasticsearch配置
spring.elasticsearch.rest.uris=192.168.42.172:9200,192.168.42.173:9200,192.168.42.174:9200
#ds访问url
dsapiurl=http://ds.emoney.cn
#上传文件接口
fileurl=http://cmsservice.emoney.cn/comm/fileuploadEdf.ashx
#后台通行证访问接口
getuserinfourl=http://adminds.emoney.cn:83/AdminPassport/api/Safe.GetUserInfo?appid=10199&PassportId={0}
checkticketurl=http://adminds.emoney.cn:83/AdminPassport/api/Ticket.ValidateClientTicket?appid=10199&ticket={0}
#根据用户em or 手机号获取用户uid
getuseruidurl=http://172.28.1.146/api/roboadvisor/1.0/user.getloginidbyname
#根据accode获取物流包基本信息
getactivityurl=http://172.28.1.146/api/logistics/1.0/activity.getactivitydetailbyactcode
#后台登录链接
loginurl=http://adminds.emoney.cn:83/AdminUserCenter/Login/Index?appUrl=http://{0}:{1}/login&FromApp=10199
#WebApiEncryptKey
apiencryptkey=c049f2a1-58aa-4855-bb4a-31286aa7fda3